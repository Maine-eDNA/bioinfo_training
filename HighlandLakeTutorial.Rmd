---
title: "Highland Lake Tutorial"
author: "Rene Francolini"
date: "11/21/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Set Up CyVerse Working Environment

create the following folders in your environment:
```
mkdir raw_reads
mkdir trimmed
mkdir filtered
```

## Cutadapt Command Line

the following lines should be run in the terminal window:
(copies the raw reads to your working directory)
(creates a document with the sample names to loop through)

```
cp work/5K_Highland_18S/* raw_reads

cd raw_reads

ls *_R1_001.fastq.gz | cut -f1 -d "R" > samples
```

the following lines should be run in the terminal at the same time:
(runs cutadapt for all files)

```
for sample in $(cat samples)
do

echo "On sample: $sample"
    
cutadapt -a ^CYGCGGTAATTCCAGCTC...CRAAGAYGATYAGATACCRT -A ^AYGGTATCTRATCRTCTTYG...GAGCTGGAATTACCGCRG -m 150 -M 550 --discard-untrimmed -o ${sample}R1_001_trimmed.fastq.gz -p ${sample}R2_001_trimmed.fastq.gz ${sample}R1_001.fastq.gz ${sample}R2_001.fastq.gz >> Env_samples_trimming_stats.txt 2>&1

done

```

the following lines should be run in the terminal window:
(copies the trimmed reads to your trimmed directory)

```
mv *trimmed* ../trimmed/

```


## Setting Up R Environment


``` {R environment, message = FALSE}
library(dada2, quietly = TRUE)

packageVersion("dada2")

```


``` {R filenames}
path <- "trimmed" #path where the cutadapted files are
list.files(path)

forward_reads <- sort(list.files(path, pattern="_R1_001_trimmed.fastq.gz", full.names = TRUE))

reverse_reads <- sort(list.files(path, pattern="_R2_001_trimmed.fastq.gz", full.names = TRUE))


samples <- sapply(strsplit(basename(forward_reads), "_R"), `[`, 1)

```


## Quality Plot Inspection


```{r plotQuality}
#currently only running the first 4 of the list to save time
plotQualityProfile(forward_reads[1:4])
plotQualityProfile(reverse_reads[1:4])
```


## Filter and Trimming


```{r filterNames}
filterpath <- "filtered/" #where our filtered files will live
filtered_reverse_reads <- paste0(filterpath, samples, "_R2_filtered.fq.gz")
filtered_forward_reads <- paste0(filterpath, samples, "_R1_filtered.fq.gz")
```

```{r filterAndTrim}
filtered_out <- filterAndTrim(forward_reads, 
                              filtered_forward_reads,
                              reverse_reads, 
                              filtered_reverse_reads, 
                              maxEE=c(2,2),
                              minLen=175, 
                              truncLen=c(250,200))
```

```{r ViewFiltered}
filtered_out
```

```{r plotQualityFiltered}
plotQualityProfile(filtered_forward_reads[1:4])
plotQualityProfile(filtered_reverse_reads[1:4])
```


## Generate Error Model


```{r errorModel}
err_forward_reads <- learnErrors(filtered_forward_reads)
err_reverse_reads <- learnErrors(filtered_reverse_reads)

#set multithread = TRUE if running on your own system:
#err_forward_reads <- learnErrors(filtered_forward_reads, multithread = TRUE)
#err_reverse_reads <- learnErrors(filtered_reverse_reads, multithread = TRUE)
```

```{r plotErrors}
plotErrors(err_forward_reads, nominalQ=TRUE)
plotErrors(err_reverse_reads, nominalQ=TRUE)
```

## Dereplication

```{r dereplication}
derep_forward <- derepFastq(filtered_forward_reads, verbose=TRUE)
names(derep_forward) <- samples 
derep_reverse <- derepFastq(filtered_reverse_reads, verbose=TRUE)
names(derep_reverse) <- samples
```

# Inferring ASVs

```{r inferASV}
#dada_forward <- dada(derep_forward, err=err_forward_reads, #pool="pseudo")
#dada_reverse <- dada(derep_reverse, err=err_reverse_reads, #pool="pseudo")

#set multithread = TRUE if running on your own system:
dada_forward <- dada(derep_forward, err=err_forward_reads, pool="pseudo", multithread = TRUE)
dada_reverse <- dada(derep_reverse, err=err_reverse_reads, pool="pseudo", multithread = TRUE)
```


## Merging paired reads

```{r merge}
merged_amplicons <- mergePairs(dada_forward, 
                              derep_forward, 
                              dada_reverse,
                              derep_reverse, 
                              minOverlap=20)
```


## Count Table and Summary

```{r seqtab}
seqtab <- makeSequenceTable(merged_amplicons)
dim(seqtab)
```

```{r removeChimeras}
seqtab.nochim <- removeBimeraDenovo(seqtab, multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)

sum(seqtab.nochim)/sum(seqtab)

write.csv(seqtab.nochim, "seqtab-nochim.csv")
```

```{r readCounts}
getN <- function(x) sum(getUniques(x))


summary_tab <- data.frame(row.names=samples, dada2_input=filtered_out[,1],
               filtered=filtered_out[,2], dada_f=sapply(dada_forward, getN),
               dada_r=sapply(dada_reverse, getN), merged=sapply(merged_amplicons, getN),
               nonchim=rowSums(seqtab.nochim),
               final_perc_reads_retained=round(rowSums(seqtab.nochim)/filtered_out[,1]*100, 1))

write.table(summary_tab, "read-count-tracking.tsv", quote=FALSE, sep="\t", col.names=NA)
```


## ASV Tables

``` {R ASVFastaFile}
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode="character")

for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep="_")
}

asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, "ASVs.fa")
```

``` {R CountTable}
asv_tab <- t(seqtab.nochim)
row.names(asv_tab) <- sub(">", "", asv_headers)
write.table(asv_tab, "ASVs_counts.tsv", sep="\t", quote=F, col.names=NA)
```


## Assign Taxonomy

the following lines should be run in the terminal window:
(changes your working directory away from raw_reads and to home)
(gets the pr2 taxonomic dataset from git using wget)

```
cd ../

wget https://github.com/pr2database/pr2database/releases/download/v4.14.0/pr2_version_4.14.0_SSU_dada2.fasta.gz
```

(now back in the R Console)
``` {R Assign Taxonomy}
taxa <- assignTaxonomy(seqtab.nochim, "pr2_version_4.14.0_SSU_dada2.fasta.gz", multithread=T,minBoot=50)

rownames(taxa) <- gsub(pattern=">", replacement="", x=asv_headers)

write.csv(taxa, "ASV_taxa.csv")
```


## Phun with Phyloseq

``` {R PhyloseqObj}

library(phyloseq)
library(ggplot2)

info <- read.table("info_18S.txt", header=T,sep="\t")
rownames(info) <- rownames(seqtab.nochim)

rawotus <- phyloseq(otu_table(asv_tab, taxa_are_rows=T), 
                    sample_data(info), 
                    tax_table(as.matrix(taxa)))

rawotus@sam_data
nsamples(rawotus)
head(rawotus@otu_table)
head(rawotus@tax_table)

#remove any ASVs that appear <20x per sample
ps_filt_samples <- prune_samples(sample_sums(rawotus)>=20, rawotus)
#ASVs must appear in more than one sample and in that sample have a count greater than 2
wh0 <-  genefilter_sample(ps_filt_samples, filterfun_sample(function(x) x > 2), A=2)
ps <- prune_taxa(wh0, ps_filt_samples)

```



``` {R PlotTop20}
top20 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:200]
ps.top20 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top20 <- prune_taxa(top20, ps.top20)
#ps.top20@sam_data

p <- plot_bar(ps.top20, x="JulDay", fill="Phylum") +
  theme(text = element_text(size = 14) , legend.position = "right") +
  scale_x_continuous(breaks=c(186,192,195,199,205,212,219,227,233,241,255,268,285))+
  facet_wrap(~Layer)
p
```



